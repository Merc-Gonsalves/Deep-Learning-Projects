{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import t4utils as t4\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=5, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of steps to implement a deep neural network model:\n",
    "\n",
    "1. Initialize the parameters of a $K$-layer neural network.\n",
    "2. Implement the forward propagation:\n",
    "    - compute linear component $z^{[k]}=a^{[k-1]} W^{[k]} + b^{[k]}$\n",
    "    - compute activation $a^{[k]} = g(z^{[k]})$, where $g(\\cdot)$ is the layer non-linearity (activation function). I'll use both ReLU and sigmoid.\n",
    "    - combine $K-1$ ReLU layers with a final sigmoid layer ($K$ layer in total)\n",
    "    - compute all layer outputs\n",
    "    - evaluate loss from final layer activation\n",
    "3. Implement the backward propagation:\n",
    "    - compute the gradients for the linear component and the non-linear activation function\n",
    "    - perform backprop on sigmoid layer followed by $K-1$ ReLU layers\n",
    "    - evaluate gradients for all parameters $W^{[k]}, b^{[k]}$\n",
    "4. Update parameters using learning rate\n",
    "\n",
    "To implement our learning algorithm, we will associate a backprop module for every forward function. At every step of your \n",
    "forward evaluation, we will store results in a cache required to compute gradients during backprop. As a result, training a network using backprop requires about twice more memory than inference. With deep learning frameworks, your generally describe your network as graph and differentiation is performed automatically for training model parameters.\n",
    "\n",
    "## A. Load Dataset and Initialise Model ##\n",
    "\n",
    "First, generating a dataset to work on. We will use the `catsvsnoncats` image dataset used in the logistic regression notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath='datasets/catsvsnoncats.h5'):\n",
    "    \"\"\"\n",
    "    Load and pre-process dataset\n",
    "    \n",
    "    Arguments:\n",
    "    filepath -- string, dataset path\n",
    "\n",
    "    Returns:\n",
    "    (X_train, Y_train), (X_test, Y_test), classes -- training and test datasets\n",
    "    \"\"\"\n",
    "\n",
    "    h5file = h5py.File(filepath, \"r\")\n",
    "    X_train = np.array(h5file[\"X_train\"][:])\n",
    "    Y_train = np.array(h5file[\"Y_train\"][:])\n",
    "    X_test = np.array(h5file[\"X_test\"][:])\n",
    "    Y_test = np.array(h5file[\"Y_test\"][:])\n",
    "    classes = np.array(h5file[\"Classes\"][:]) \n",
    "    h5file.close()\n",
    "\n",
    "    # Reshape and scale datasets containing N_train and N_test mages such that\n",
    "    # X_train.shape = (n_train, W * H * 3)\n",
    "    # Y_train.shape = (n_train, 1)\n",
    "    # X_test.shape = (n_test, W * H * 3)\n",
    "    # X_test.shape = (n_test, 1)\n",
    "    # Scale X_train and X_test from range {0..255} to (0,1)\n",
    "    \n",
    "    X_train = X_train.reshape(X_train.shape[0],-1)/255\n",
    "    X_test = X_test.reshape(X_test.shape[0],-1)/255\n",
    "    Y_train = Y_train.reshape(Y_train.shape[0],1)\n",
    "    Y_test = Y_test.reshape(Y_test.shape[0],1)\n",
    "\n",
    "    return (X_train, Y_train), (X_test, Y_test), classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring the model topology i.e. K-1 ReLU activated layers and one final sigmoid activated layer. Initialise bias to zeros and weights from $\\mathcal{N}(0,\\sigma)$ with $\\sigma = 1 /\\sqrt{n_h^{[k]}}$ where $n_h^{[k]}$ is number of units in the $k$-th layer.\n",
    "\n",
    "The number of units of the first layer is determined by the number of features of our dataset, $x$. Similarly the number of units of the final output layer is determined by the dimension of $y$. The number of units for the remaining $K-1$ hidden layers is specified as an array of integers.\n",
    "\n",
    " Setup the model topology\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config(X, Y, n_h=[128, 64, 16, 8]):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- n data samples, shape = (n, n_x)\n",
    "    Y -- ground truth label, column vector of shape (n, n_y)\n",
    "    n_h -- array with number of units in hidden layers, size K-1\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing initialised model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised bias vector of shape (1, n_h1)\n",
    "        ...\n",
    "        Wk -- initialised weight matrix of shape (n_hk-1, n_hk)\n",
    "        bk -- initialised bias vector of shape (1, n_hk)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised bias vector of shape (1, n_y)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = X.shape[1] # size of input layer\n",
    "    n_y = Y.shape[1] # size of output layer\n",
    "    \n",
    "    dims = sum([[n_x], n_h, [n_y]], [])\n",
    "    K = len(dims) # number of network layers\n",
    "\n",
    "    params = {}\n",
    "    \n",
    "    for k in range(1, K):\n",
    "        params['W{}'.format(k)] = np.random.randn(dims[k-1],dims[k])/np.sqrt(dims[k - 1])\n",
    "        params['b{}'.format(k)] = np.zeros((1,dims[k]))\n",
    "                                                    \n",
    "        assert(params['W{}'.format(k)].shape == (dims[k - 1], dims[k]))\n",
    "        assert(params['b{}'.format(k)].shape == (1, dims[k]))\n",
    "\n",
    "    assert(X.shape[0] == (Y.shape[0]))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.9195  -0.21968  0.58   ]\n",
      " [-0.28855 -0.51552 -0.41944]\n",
      " [ 0.15847  0.17663  0.75786]\n",
      " [-0.18893 -0.54634 -0.32417]]\n",
      "b1 = [[0. 0. 0.]]\n",
      "W2 = [[ 0.85447 -1.2155 ]\n",
      " [ 0.13204  0.20821]\n",
      " [-0.83132 -0.38261]]\n",
      "b2 = [[0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X, Y, n_h = t4.model_config_test()\n",
    "params = model_config(X, Y, [3])\n",
    "for k in range(1, len(params) // 2 + 1):\n",
    "    print(\"W{} = {}\".format(k, params['W{}'.format(k)]))\n",
    "    print(\"b{} = {}\".format(k, params['b{}'.format(k)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation for linearity\n",
    "def linear_fwd(W, b, A):\n",
    "    \"\"\"\n",
    "    Linearity\n",
    "\n",
    "    Arguments:\n",
    "    W -- weight matrix, shape (n_hk-1, n_hk)\n",
    "    b -- bias row vector, shape (1, n_hk)\n",
    "    A -- input, shape (n, n_hk-1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- linear output, shape (n, n_hk)\n",
    "    cache -- dictionary for back propagation\n",
    "        W -- weight matrix\n",
    "        b -- bias row vector\n",
    "        A_prev -- input\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = A @ W + b\n",
    "    cache = {'W':W, 'b':b, 'A_prev':A}\n",
    "    \n",
    "    assert(Z.shape == (A.shape[0], W.shape[1]))\n",
    "    return Z, cache\n",
    "\n",
    "# Forward propagation for sigmoid non-linearity\n",
    "def sigmoid_fwd(Z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(Z), same shape as Z\n",
    "    cache -- dictionary for backpropagation\n",
    "        Z -- activation's input\n",
    "    \"\"\"\n",
    "\n",
    "    A = 1/(1 + np.exp(-Z))\n",
    "    cache = {'Z':Z}\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, cache\n",
    "\n",
    "# Forward propagation for ReLU non-linearity\n",
    "def relu_fwd(Z):\n",
    "    \"\"\"\n",
    "    RELU activation\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of ReLU(Z), same shape as Z\n",
    "    cache -- dictionary for backpropagation\n",
    "        Z -- activation's input\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0, Z)\n",
    "    cache = {'Z':Z}\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z.T = [[0.71591 1.45199]]\n",
      "A.T = ReLU(Z.T) = [[0.71591 1.45199]]\n",
      "A.T = sigmoid(Z.T) = [[0.67171 0.8103 ]]\n"
     ]
    }
   ],
   "source": [
    "W, b, A = t4.linear_fwd_test()\n",
    "Z, cache = linear_fwd(W, b, A)\n",
    "print(\"Z.T = {}\".format(Z.T))\n",
    "A, cache = relu_fwd(Z)\n",
    "print(\"A.T = ReLU(Z.T) = {}\".format(A.T))\n",
    "A, cache = sigmoid_fwd(Z)\n",
    "print(\"A.T = sigmoid(Z.T) = {}\".format(A.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer forward propagation for \n",
    "def singlelayer_fwd(W, b, A_prev, non_linearity='ReLU'):\n",
    "    \"\"\"\n",
    "    Single layer forward propagation (linear + non-linearity)\n",
    "\n",
    "    Arguments:\n",
    "    W -- weight matrix, shape (n_hk-1, n_hk)\n",
    "    b -- bias row vector, shape (1, n_hk)\n",
    "    A_prev -- input, shape (n, n_hk-1)\n",
    "    non_linearity -- string ('ReLU' or 'Sigmoid') activation for layer\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of g(A_prev @ W + b), shape (n, n_hk)\n",
    "    cache -- dictionary for backprop\n",
    "        LINEAR -- dictionary linear cache\n",
    "        ACTIVATION -- dictionary activation cache        \n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = linear_fwd(W,b,A_prev)\n",
    "    \n",
    "    if non_linearity == 'ReLU':\n",
    "        A, activation_cache = relu_fwd(Z)\n",
    "    elif non_linearity == 'Sigmoid':\n",
    "        A, activation_cache = sigmoid_fwd(Z)\n",
    "    \n",
    "    assert(A.shape == (A_prev.shape[0], W.shape[1]))\n",
    "    return A, {'LINEAR': linear_cache, 'ACTIVATION': activation_cache}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_Sigmoid.T = [[0.67171 0.8103 ]]\n",
      "cache = {'LINEAR': {'W': array([[ 0.57376],\n",
      "       [ 0.28773],\n",
      "       [-0.23563]]), 'b': array([[0.95349]]), 'A_prev': array([[-0.21768,  0.82146,  1.48128],\n",
      "       [ 1.33186, -0.36187,  0.68561]])}, 'ACTIVATION': {'Z': array([[0.71591],\n",
      "       [1.45199]])}}\n",
      "A_ReLU.T = [[0.71591 1.45199]]\n",
      "cache = {'LINEAR': {'W': array([[ 0.57376],\n",
      "       [ 0.28773],\n",
      "       [-0.23563]]), 'b': array([[0.95349]]), 'A_prev': array([[-0.21768,  0.82146,  1.48128],\n",
      "       [ 1.33186, -0.36187,  0.68561]])}, 'ACTIVATION': {'Z': array([[0.71591],\n",
      "       [1.45199]])}}\n"
     ]
    }
   ],
   "source": [
    "W, b, A_prev = t4.singlelayer_fwd_test()\n",
    "A, cache = singlelayer_fwd(W, b, A_prev, 'Sigmoid')\n",
    "print(\"A_Sigmoid.T = {}\".format(A.T))\n",
    "print(\"cache = {}\".format(cache))\n",
    "A, cache = singlelayer_fwd(W, b, A_prev, 'ReLU')\n",
    "print(\"A_ReLU.T = {}\".format(A.T))\n",
    "print(\"cache = {}\".format(cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation (inference)\n",
    "def forward_prop(params, X, Y=None):\n",
    "    \"\"\"\n",
    "    Compute the layer activations and loss if needed\n",
    "\n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    X -- n data samples, shape = (n, n_x)\n",
    "    Y -- optional argument, ground truth label, column vector of shape (n, n_y)\n",
    "\n",
    "    Returns:\n",
    "    A -- final layer output (activation value) \n",
    "    loss -- cross-entropy loss or NaN if Y=None\n",
    "    caches -- array of caches for the K layers\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    K = len(params) >> 1\n",
    "    A = X\n",
    "    \n",
    "    # K-1 [Linear->ReLU] layer\n",
    "    for k in range(1, K):\n",
    "        A_prev = A\n",
    "        W = params['W{}'.format(k)]\n",
    "        b = params['b{}'.format(k)]\n",
    "        A, cache = singlelayer_fwd(W,b,A_prev)\n",
    "        caches.append(cache)\n",
    "\n",
    "    # 1 [Linear->Sigmoid] layer\n",
    "    A_prev = A\n",
    "    W = params['W{}'.format(K)]\n",
    "    b = params['b{}'.format(K)]\n",
    "    A, cache = singlelayer_fwd(W, b, A_prev,'Sigmoid')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    loss = float('nan')\n",
    "    if Y is not None:\n",
    "        Y_hat = A\n",
    "        n = Y.shape[0]\n",
    "        # Compute the cross-entropy loss\n",
    "        loss = -(1/n)*np.sum(Y*np.log(Y_hat) + (1-Y)*np.log(1-Y_hat))\n",
    "\n",
    "        loss = np.squeeze(loss)\n",
    "        assert(loss.dtype == float)\n",
    "        \n",
    "    assert(A.shape == (X.shape[0], W.shape[1]))\n",
    "    return A, loss, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.T = [[0.10101 0.33321 0.01484 0.00042]]\n",
      "loss = 0.67831\n",
      "3 caches\n"
     ]
    }
   ],
   "source": [
    "X, Y, params = t4.forward_prop_test()\n",
    "A, loss, caches = forward_prop(params, X, Y)\n",
    "print(\"A.T = {}\".format(A.T))\n",
    "print(\"loss = {:.5f}\".format(loss))\n",
    "print(\"{} caches\".format(len(caches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation for linearity\n",
    "def linear_back(dZ, cache):\n",
    "    \"\"\"\n",
    "    Linearity backprop\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- gradient of loss with respect to current layer linear output\n",
    "    cache -- dictionary from forward propagation\n",
    "        W -- weight matrix\n",
    "        b -- bias row vector\n",
    "        A_prev -- previous layer activation input\n",
    "\n",
    "    Returns:\n",
    "    dW -- gradient of loss with respect to current layer weights\n",
    "    db -- gradient of loss with respect to current layer bias\n",
    "    dA_prev -- gradient of loss with respect to activation of previous layer output\n",
    "    \"\"\"\n",
    "\n",
    "    W = cache['W']\n",
    "    b = cache['b']\n",
    "    A_prev = cache['A_prev']\n",
    "    n = A_prev.shape[0]\n",
    "    dW = (1/n)*A_prev.T @ dZ\n",
    "    db = (1/n)*np.sum(dZ, axis=0, keepdims=True)\n",
    "    dA_prev = dZ @ W.T\n",
    "    \n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dW, db, dA_prev\n",
    "\n",
    "# Backward propagation for ReLU non-linearity\n",
    "def relu_back(dA, cache):\n",
    "    \"\"\"\n",
    "    ReLU backprop\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of loss with respect to activation\n",
    "    cache -- dictionary from forward propagation\n",
    "        Z -- layer linearity output \n",
    "\n",
    "    Returns:\n",
    "    dZ -- gradient of loss with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache['Z']\n",
    "    dZ = np.array(dA,copy=True)\n",
    "    dZ[Z<=0]=0\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "\n",
    "# Backward propagation for sigmoid non-linearity\n",
    "def sigmoid_back(dA, cache):\n",
    "    \"\"\"\n",
    "    Sigmoid backprop\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of loss with respect to activation\n",
    "    cache -- dictionary from forward propagation\n",
    "        Z -- layer linearity output \n",
    "\n",
    "    Returns:\n",
    "    dZ -- gradient of loss with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache['Z']\n",
    "    S = 1/(1+np.exp(-Z))\n",
    "    dZ = dA*S*(1-S)\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW.T = [[0.12038 0.0907  0.15756]]\n",
      "db = [[0.30189]]\n",
      "dA_prev = [[ 0.05129 -0.20755  0.3678 ]\n",
      " [-0.19356  0.78325 -1.38795]]\n",
      "ReLU: dZ = [[-0.21768  0.82146  0.     ]\n",
      " [ 1.33186  0.       0.     ]]\n",
      "Sigmoid: dZ = [[-0.05018  0.20117  0.36523]\n",
      " [ 0.26743 -0.0476   0.1664 ]]\n"
     ]
    }
   ],
   "source": [
    "dZ, cache = t4.linear_back_test()\n",
    "dW, db, dA_prev = linear_back(dZ, cache)\n",
    "print(\"dW.T = {}\".format(dW.T))\n",
    "print(\"db = {}\".format(db))\n",
    "print(\"dA_prev = {}\".format(dA_prev))\n",
    "A, cache = t4.non_linearity_test()\n",
    "dZ = relu_back(A, cache)\n",
    "print(\"ReLU: dZ = {}\".format(dZ))\n",
    "dZ = sigmoid_back(A, cache)\n",
    "print(\"Sigmoid: dZ = {}\".format(dZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singlelayer_back(dA, cache, non_linearity='ReLU'):\n",
    "    \"\"\"\n",
    "    Single layer backprop (linear + non-linearity)\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of loss with respect to activation\n",
    "    cache -- dictionary from forward propagation\n",
    "        LINEAR -- dictionary from forward linear propagation \n",
    "        ACTIVATION -- dictionary from forward non-linearity propagation \n",
    "    non_linearity -- string ('ReLU' or 'Sigmoid') activation for layer\n",
    "\n",
    "    Returns:\n",
    "    dW -- gradient of loss with respect to current layer weights\n",
    "    db -- gradient of loss with respect to current layer bias\n",
    "    dA_prev -- gradient of loss with respect to activation of previous layer output\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache = cache['LINEAR']\n",
    "    activation_cache = cache['ACTIVATION']\n",
    "    \n",
    "    if non_linearity == 'Sigmoid':\n",
    "        dZ = sigmoid_back(dA, activation_cache)\n",
    "    elif non_linearity == 'ReLU':\n",
    "        dZ = relu_back(dA, activation_cache)\n",
    "        \n",
    "    dW, db, dA_prev = linear_back(dZ, linear_cache)\n",
    "    \n",
    "    return dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU: dW.T = [[-0.16122 -0.14496  0.03939]]\n",
      "ReLU: db = [[-0.10884]]\n",
      "ReLU: dA_prev = [[ 0.05129 -0.20755  0.3678 ]\n",
      " [ 0.       0.       0.     ]]\n",
      "Sigmoid: dW.T = [[0.02563 0.01894 0.03751]]\n",
      "Sigmoid: db = [[0.06896]]\n",
      "Sigmoid: dA_prev = [[ 0.01282 -0.05188  0.09194]\n",
      " [-0.04532  0.18338 -0.32496]]\n"
     ]
    }
   ],
   "source": [
    "dA, cache = t4.singlelayer_back_test()\n",
    "\n",
    "dW, db, dA_prev = singlelayer_back(dA, cache, 'ReLU')\n",
    "print(\"ReLU: dW.T = {}\".format(dW.T))\n",
    "print(\"ReLU: db = {}\".format(db))\n",
    "print(\"ReLU: dA_prev = {}\".format(dA_prev))\n",
    "\n",
    "dW, db, dA_prev = singlelayer_back(dA, cache, 'Sigmoid')\n",
    "print(\"Sigmoid: dW.T = {}\".format(dW.T))\n",
    "print(\"Sigmoid: db = {}\".format(db))\n",
    "print(\"Sigmoid: dA_prev = {}\".format(dA_prev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward_propagation\n",
    "def back_prop(AK, Y, caches):\n",
    "    \"\"\"\n",
    "    Compute back-propagation gradients\n",
    "    \n",
    "    Arguments:\n",
    "    AK -- probability vector, final layer output, shape (1, n_y)\n",
    "    Y -- ground truth output (n, n_y)\n",
    "    caches -- array of layer cache, len=K\n",
    "    \n",
    "    Returns:\n",
    "    grads -- dictionary containing your gradients with respect to all parameters\n",
    "        dW1 -- weight gradient matrix of shape (n_x, n_h1)\n",
    "        db1 -- bias gradient vector of shape (1, n_h1)\n",
    "        ...\n",
    "        dWK -- weight gradient matrix of shape (n_hK-1, n_y)\n",
    "        dbK -- bias gradient vector of shape (1, n_y)\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    K = len(caches)\n",
    "    n = AK.shape[0]\n",
    "    assert(Y.shape == AK.shape)\n",
    "\n",
    "    dAK = -((Y/AK) - ((1-Y) / (1-AK)))\n",
    "    cache = caches[K-1] \n",
    "    grads[\"dW{}\".format(K)], grads[\"db{}\".format(K)],  grads[\"dA{}\".format(K)] = singlelayer_back(dAK, cache, non_linearity='Sigmoid')\n",
    "    \n",
    "    for k in reversed(range(K - 1)):\n",
    "        cache = caches[k]\n",
    "        grads[\"dW{}\".format(k + 1)], grads[\"db{}\".format(k + 1)], grads[\"dA{}\".format(k + 1)] = singlelayer_back(grads[\"dA{}\".format(k+2)], cache)\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1.T = [[ 0.39632  0.35635 -0.09682  0.18344]\n",
      " [ 0.71849  0.64602 -0.17552  0.33255]\n",
      " [ 0.16057  0.14437 -0.03923  0.07432]]\n",
      "db1 = [[0.26756 0.48505 0.1084 ]]\n",
      "dA1 = [[-1.2351  -0.07942  2.06421 -1.18989]\n",
      " [ 0.       0.       0.       0.     ]]\n",
      "dW2.T = [[-0.68133  0.1135  -0.30305]]\n",
      "db2 = [[0.86059]]\n",
      "dA2 = [[0.53511 0.9701  0.2168 ]\n",
      " [0.47488 0.86091 0.1924 ]]\n"
     ]
    }
   ],
   "source": [
    "AK, Y, caches = t4.back_prop_test()\n",
    "grads = back_prop(AK, Y, caches)\n",
    "print(\"dW1.T = {}\".format(grads['dW1'].T))\n",
    "print(\"db1 = {}\".format(grads['db1']))\n",
    "print(\"dA1 = {}\".format(grads['dA1']))\n",
    "print(\"dW2.T = {}\".format(grads['dW2'].T))\n",
    "print(\"db2 = {}\".format(grads['db2']))\n",
    "print(\"dA2 = {}\".format(grads['dA2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model parameters\n",
    "def update_params(params, grads, learning_rate=0.8):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    grads -- dictionary containing gradients\n",
    "        dW1 -- weight gradient matrix of shape (n_x, n_h1)\n",
    "        db1 -- bias gradient vector of shape (1, n_h1)\n",
    "        ...\n",
    "        dWK -- weight gradient matrix of shape (n_hK-1, n_y)\n",
    "        dbK -- bias gradient vector of shape (1, n_y)\n",
    "    learning_rate -- learning rate of the gradient descent (hyperparameter)\n",
    "\n",
    "    Returns:\n",
    "    params -- dictionary containing updated parameters\n",
    "    \"\"\"\n",
    "\n",
    "    K = len(params) >> 1\n",
    "    for k in range(1, K + 1):\n",
    "        params['W{}'.format(k)] = params['W{}'.format(k)] - learning_rate*grads['dW{}'.format(k)]\n",
    "        params['b{}'.format(k)] = params['b{}'.format(k)] - learning_rate*grads['db{}'.format(k)]\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.T = [[-0.39654  1.51821  0.58204  1.00121]\n",
      " [ 0.7778  -0.33413  0.35043 -1.55824]\n",
      " [ 1.47163  0.72108 -0.23125 -0.4334 ]]\n",
      "b1 = [[-0.07123 -0.68594  0.23951]]\n",
      "W2.T = [[-0.14884  2.72671  0.61945]]\n",
      "b2 = [[0.74769]]\n"
     ]
    }
   ],
   "source": [
    "params, grads = t4.update_params_test()\n",
    "params = update_params(params, grads, 0.1)\n",
    "print(\"W1.T = {}\".format(params['W1'].T))\n",
    "print(\"b1 = {}\".format(params['b1']))\n",
    "print(\"W2.T = {}\".format(params['W2'].T))\n",
    "print(\"b2 = {}\".format(params['b2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter optimisation using backprop\n",
    "def model_fit(params, X, Y, epochs=2500, learning_rate=0.0075, verbose=False):\n",
    "    \"\"\"\n",
    "    Optimise model parameters by performing gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    X -- n data samples  (n, n_x)\n",
    "    Y -- groud truth label vector of size (n, n_y)\n",
    "    epochs -- number of iteration updates through dataset\n",
    "    learning_rate -- learning rate of the gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary with optimised parameters\n",
    "    grads -- dictionary with final gradients\n",
    "    loss_log -- list of loss values for every 100 updates\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_log = []\n",
    "    for i in range(epochs):\n",
    "        A, loss, caches = forward_prop(params,X,Y)\n",
    "        grads = back_prop(A,Y,caches)\n",
    "        params = update_params(params,grads,learning_rate)\n",
    "        \n",
    "        # logs\n",
    "        if i % 100 == 0:\n",
    "            loss_log.append(loss.item())\n",
    "            if verbose:\n",
    "                print(\"Loss after {} epochs: {:.3f}\".format(i, loss))\n",
    "     \n",
    "    return params, grads, loss_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = [0.67831 0.39311 0.38196]\n",
      "W1.T = [[-0.93729  0.72484 -0.71946  0.90889 -0.95217]\n",
      " [ 0.06497  0.47414 -0.83891  0.21742  0.58681]\n",
      " [-3.22537  0.40678 -0.81217 -1.18205  1.06676]\n",
      " [ 1.02805  0.46416 -0.46209 -0.91687  0.2003 ]]\n",
      "b1 = [[-0.89367 -0.9791  -1.53733 -1.24609]]\n",
      "W2.T = [[ 0.33411 -0.79252  1.2068  -0.83628]\n",
      " [-0.17309  1.839   -0.57709  0.31693]\n",
      " [ 1.46725 -0.43936 -1.03104  0.35325]]\n",
      "b2 = [[ 1.44688 -0.37785 -1.12293]]\n",
      "W3.T = [[-0.48626  1.47999 -2.10593]]\n",
      "b3 = [[0.35461]]\n",
      "dW1.T = [[-0.00422  0.00449  0.03353  0.02765 -0.00591]\n",
      " [ 0.       0.       0.       0.       0.     ]\n",
      " [ 0.01105  0.00226 -0.00011  0.00337 -0.0016 ]\n",
      " [ 0.       0.       0.       0.       0.     ]]\n",
      "db1 = [[ 0.01863  0.      -0.00654  0.     ]]\n",
      "dW2.T = [[ 0.02712  0.      -0.02508  0.     ]\n",
      " [ 0.       0.       0.       0.     ]\n",
      " [-0.00808  0.       0.       0.     ]]\n",
      "db2 = [[ 0.01661  0.      -0.00378]]\n",
      "dW3.T = [[-0.00584  0.       0.00361]]\n",
      "db3 = [[-0.03416]]\n"
     ]
    }
   ],
   "source": [
    "X, Y, params = t4.forward_prop_test()\n",
    "params, grads, loss_log = model_fit(params, X, Y, epochs = 300, verbose=False)\n",
    "print(\"loss = {}\".format(np.array(loss_log)))\n",
    "print(\"W1.T = {}\".format(params['W1'].T))\n",
    "print(\"b1 = {}\".format(params['b1']))\n",
    "print(\"W2.T = {}\".format(params['W2'].T))\n",
    "print(\"b2 = {}\".format(params['b2']))\n",
    "print(\"W3.T = {}\".format(params['W3'].T))\n",
    "print(\"b3 = {}\".format(params['b3']))\n",
    "print(\"dW1.T = {}\".format(grads['dW1'].T))\n",
    "print(\"db1 = {}\".format(grads['db1']))\n",
    "print(\"dW2.T = {}\".format(grads['dW2'].T))\n",
    "print(\"db2 = {}\".format(grads['db2']))\n",
    "print(\"dW3.T = {}\".format(grads['dW3'].T))\n",
    "print(\"db3 = {}\".format(grads['db3']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model inference\n",
    "def model_predict(params, X):\n",
    "    \"\"\"\n",
    "    Predict class label using model parameters\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    X -- n data samples, shape (n, n_x)\n",
    "    \n",
    "    Returns:\n",
    "    Y_hat -- vector with class predictions for examples in X, shape (n, n_y)\n",
    "    \"\"\"\n",
    "        \n",
    "    AK, _, _ = forward_prop(params, X)\n",
    "    Y_hat = (AK > 0.5)*1 # Convert activations to {0,1} predictions\n",
    "        \n",
    "    n = X.shape[0]\n",
    "    assert(Y_hat.shape == (n, 1))    \n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.T = [[0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X, _, params = t4.forward_prop_test()\n",
    "params['b1'] = np.zeros((1,4))\n",
    "Y_hat = model_predict(params, X)\n",
    "print(\"predictions.T = {}\".format(Y_hat.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep model\n",
    "def deep_model(X_train, Y_train, X_test, Y_test, hidden_layers=[21, 7, 3], epochs=2500, learning_rate=0.007, verbose=True):\n",
    "    '''\n",
    "    Build, train and evalaute the K-layer model\n",
    "    (K-1) * [LINEAR -> RELU]  -> [LINEAR -> SIGMOID] \n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set a numpy array of shape (n_train, n_x)\n",
    "    Y_train -- training groud truth vector of size (n_train, n_y)\n",
    "    X_test -- testing set a numpy array of shape (n_test, n_x)\n",
    "    Y_test -- testing groud truth vector of size (n_test, n_y)\n",
    "    hidden_layers -- array with number of units in hidden layers\n",
    "    epochs -- number of iteration updates through dataset for training (hyperparameter)\n",
    "    learning_rate -- learning rate of the gradient descent (hyperparameter)\n",
    "    \n",
    "    Returns:\n",
    "    model -- dictionary \n",
    "        PARAMS -- parameters\n",
    "        LOSS -- log of training loss\n",
    "        GRADS -- final \n",
    "        ACC -- array with training and testing accuracies\n",
    "        LR -- learning rate\n",
    "    '''\n",
    "    \n",
    "    params = model_config(X_train, Y_train, hidden_layers)\n",
    "    params, grads, loss = model_fit(params, X_train, Y_train, epochs, learning_rate, verbose)\n",
    "    Y_hat_train = model_predict(params, X_train)\n",
    "    Y_hat_test = model_predict(params, X_test)\n",
    "    train_acc = 100 - (np.mean(np.abs(Y_hat_train-Y_train))*100)\n",
    "    test_acc = 100 - (np.mean(np.abs(Y_hat_test-Y_test))*100)\n",
    "\n",
    "    print(\"{:.1f}% training acc.\".format(train_acc))\n",
    "    print(\"{:.1f}% test acc.\".format(test_acc))\n",
    "        \n",
    "    return {\"PARAMS\": params, \"LOSS\": loss, \"GRADS\": grads, \"ACC\": [train_acc, test_acc], \"LR\": learning_rate}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Training a single hidden layer network with 7 units on the `catsvsnoncats` dataset. The loss should decrease; this will take a few minutes to complete the 2500 training epochs. Monitoring the loss convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 epochs: 0.747\n",
      "Loss after 100 epochs: 0.529\n",
      "Loss after 200 epochs: 0.469\n",
      "Loss after 300 epochs: 0.428\n",
      "Loss after 400 epochs: 0.394\n",
      "Loss after 500 epochs: 0.350\n",
      "Loss after 600 epochs: 0.303\n",
      "Loss after 700 epochs: 0.273\n",
      "Loss after 800 epochs: 0.214\n",
      "Loss after 900 epochs: 0.178\n",
      "Loss after 1000 epochs: 0.141\n",
      "Loss after 1100 epochs: 0.106\n",
      "Loss after 1200 epochs: 0.090\n",
      "Loss after 1300 epochs: 0.077\n",
      "Loss after 1400 epochs: 0.067\n",
      "Loss after 1500 epochs: 0.059\n",
      "Loss after 1600 epochs: 0.052\n",
      "Loss after 1700 epochs: 0.046\n",
      "Loss after 1800 epochs: 0.042\n",
      "Loss after 1900 epochs: 0.037\n",
      "Loss after 2000 epochs: 0.034\n",
      "Loss after 2100 epochs: 0.031\n",
      "Loss after 2200 epochs: 0.028\n",
      "Loss after 2300 epochs: 0.026\n",
      "Loss after 2400 epochs: 0.024\n",
      "100.0% training acc.\n",
      "70.0% test acc.\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test), classes = load_dataset('datasets/catsvsnoncats.h5')\n",
    "np.random.seed(2019)\n",
    "model = deep_model(X_train, Y_train, X_test, Y_test, hidden_layers=[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 epochs: 0.772\n",
      "Loss after 100 epochs: 0.666\n",
      "Loss after 200 epochs: 0.642\n",
      "Loss after 300 epochs: 0.610\n",
      "Loss after 400 epochs: 0.565\n",
      "Loss after 500 epochs: 0.499\n",
      "Loss after 600 epochs: 0.434\n",
      "Loss after 700 epochs: 0.373\n",
      "Loss after 800 epochs: 0.313\n",
      "Loss after 900 epochs: 0.253\n",
      "Loss after 1000 epochs: 0.219\n",
      "Loss after 1100 epochs: 0.164\n",
      "Loss after 1200 epochs: 0.123\n",
      "Loss after 1300 epochs: 0.100\n",
      "Loss after 1400 epochs: 0.082\n",
      "Loss after 1500 epochs: 0.068\n",
      "Loss after 1600 epochs: 0.059\n",
      "Loss after 1700 epochs: 0.050\n",
      "Loss after 1800 epochs: 0.044\n",
      "Loss after 1900 epochs: 0.039\n",
      "Loss after 2000 epochs: 0.035\n",
      "Loss after 2100 epochs: 0.031\n",
      "Loss after 2200 epochs: 0.028\n",
      "Loss after 2300 epochs: 0.025\n",
      "Loss after 2400 epochs: 0.023\n",
      "100.0% training acc.\n",
      "78.0% test acc.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 258345\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test), classes = load_dataset('datasets/catsvsnoncats.h5')\n",
    "np.random.seed(2019)\n",
    "model = deep_model(X_train, Y_train, X_test, Y_test, hidden_layers=[21, 9, 7], learning_rate=0.009)\n",
    "\n",
    "plt.plot(model[\"LOSS\"])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (x100)')\n",
    "plt.title(\"Learning rate = {}\".format(model[\"LR\"]))\n",
    "plt.show()\n",
    "\n",
    "params = model[\"PARAMS\"]\n",
    "parameter_count = 0\n",
    "for key in params.keys():\n",
    "    parameter_count = parameter_count + np.prod(params[key].shape)\n",
    "print(\"Number of trainable parameters: {}\".format(parameter_count))"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
